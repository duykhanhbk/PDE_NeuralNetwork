# A deep learning algorithm for solving partial differential equations
# Universal approximation theorem
In the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of Rn, under mild assumptions on the activation function. The theorem thus states that simple neural networks can represent a wide variety of interesting functions when given appropriate parameters; however, it does not touch upon the algorithmic learnability of those parameters.
One of the first versions of the theorem was proved by George Cybenko in 1989 for sigmoid activation functions
# Approximation by Superpositions of a Sigmoidal Function
#  Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations
#  The expressive power of neural networks
#  Approximation capabilities of multilayer feedforward networks
# Deep Learning
# machine-learning
# Genetic Algorithm
